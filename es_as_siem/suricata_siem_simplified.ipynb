{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TD SIEM : Suricata, Filebeat, Elasticsearch\n",
        "\n",
        "## Objectifs\n",
        "\n",
        "Ce TD vous permettra de :\n",
        "- V√©rifier le bon fonctionnement de la stack SIEM\n",
        "- Analyser les donn√©es index√©es par Filebeat\n",
        "- D√©tecter des anomalies sans ML\n",
        "- D√©tecter des anomalies avec ML (Isolation Forest)\n",
        "\n",
        "## Pr√©requis\n",
        "\n",
        "1. D√©marrer la stack :\n",
        "```bash\n",
        "docker-compose -f docker-compose-siem.yml up -d\n",
        "```\n",
        "\n",
        "2. Attendre quelques minutes que Suricata g√©n√®re des logs et que Filebeat les indexe dans Elasticsearch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cluster Elasticsearch: green (3 n≈ìuds)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mp/code/formations/elasticsearch_cookbook/.venv/lib/python3.14/site-packages/elasticsearch/_sync/client/__init__.py:313: SecurityWarning: Connecting to 'https://localhost:9200' using TLS with verify_certs=False is insecure\n",
            "  _transport = transport_class(\n",
            "/home/mp/code/formations/elasticsearch_cookbook/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Configuration et connexion √† Elasticsearch\n",
        "from elasticsearch import Elasticsearch\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "ES_HOST = \"https://localhost:9200\"\n",
        "ES_USER = \"elastic\"\n",
        "ES_PASSWORD = \"changeme\"  # Modifiez selon votre .env\n",
        "\n",
        "# Connexion\n",
        "es = Elasticsearch(\n",
        "    [ES_HOST],\n",
        "    basic_auth=(ES_USER, ES_PASSWORD),\n",
        "    verify_certs=False\n",
        ")\n",
        "\n",
        "# V√©rification de la connexion\n",
        "health = es.cluster.health()\n",
        "print(f\"‚úÖ Cluster Elasticsearch: {health['status']} ({health['number_of_nodes']} n≈ìuds)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. V√©rification de la stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ es01\n",
            "‚úÖ es02\n",
            "‚úÖ es03\n",
            "‚úÖ kibana\n",
            "‚úÖ suricata\n",
            "‚úÖ filebeat\n",
            "\n",
            "‚úÖ Tous les services sont d√©marr√©s (6/6)\n"
          ]
        }
      ],
      "source": [
        "# V√©rification des services Docker\n",
        "services = ['es01', 'es02', 'es03', 'kibana', 'suricata', 'filebeat']\n",
        "running = []\n",
        "\n",
        "for service in services:\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['docker', 'ps', '--filter', f'name={service}', '--format', '{{.Names}}'],\n",
        "            capture_output=True, text=True, timeout=5\n",
        "        )\n",
        "        if service in result.stdout:\n",
        "            running.append(service)\n",
        "            print(f\"‚úÖ {service}\")\n",
        "        else:\n",
        "            print(f\"‚ùå {service}\")\n",
        "    except:\n",
        "        print(f\"‚ùå {service}\")\n",
        "\n",
        "if len(running) == len(services):\n",
        "    print(f\"\\n‚úÖ Tous les services sont d√©marr√©s ({len(running)}/{len(services)})\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Services d√©marr√©s: {len(running)}/{len(services)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. V√©rification de l'injection des donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Index: .ds-suricata-2026.01.17-2026.01.17-000001\n",
            "üìà Nombre de documents: 129,374\n",
            "\n",
            "üìÑ Exemple de document:\n",
            "   Type: flow\n",
            "   Timestamp: 2026-01-17T16:54:19.631Z\n",
            "   Source: 192.168.65.1:23560\n",
            "   Destination: 192.168.65.7:2376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mp/code/formations/elasticsearch_cookbook/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/home/mp/code/formations/elasticsearch_cookbook/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/home/mp/code/formations/elasticsearch_cookbook/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Recherche des index Suricata\n",
        "def get_suricata_index():\n",
        "    \"\"\"Retourne le nom de l'index Suricata le plus r√©cent\"\"\"\n",
        "    try:\n",
        "        indices = es.indices.get(index=\"suricata-*\")\n",
        "        if indices:\n",
        "            return sorted(indices.keys())[-1]\n",
        "    except:\n",
        "        pass\n",
        "    return \"suricata-*\"\n",
        "\n",
        "index_name = get_suricata_index()\n",
        "\n",
        "# Comptage des documents\n",
        "try:\n",
        "    count = es.count(index=index_name)\n",
        "    print(f\"üìä Index: {index_name}\")\n",
        "    print(f\"üìà Nombre de documents: {count['count']:,}\")\n",
        "    \n",
        "    # Exemple de document\n",
        "    if count['count'] > 0:\n",
        "        sample = es.search(index=index_name, size=1, query={\"match_all\": {}})\n",
        "        if sample['hits']['hits']:\n",
        "            doc = sample['hits']['hits'][0]['_source']\n",
        "            print(f\"\\nüìÑ Exemple de document:\")\n",
        "            print(f\"   Type: {doc.get('event_type', 'N/A')}\")\n",
        "            print(f\"   Timestamp: {doc.get('@timestamp', doc.get('timestamp', 'N/A'))}\")\n",
        "            if 'src_ip' in doc:\n",
        "                print(f\"   Source: {doc.get('src_ip')}:{doc.get('src_port', 'N/A')}\")\n",
        "                print(f\"   Destination: {doc.get('dest_ip')}:{doc.get('dest_port', 'N/A')}\")\n",
        "            if 'alert' in doc:\n",
        "                alert = doc['alert']\n",
        "                print(f\"   Alerte: {alert.get('signature', 'N/A')}\")\n",
        "                print(f\"   S√©v√©rit√©: {alert.get('severity', 'N/A')}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. D√©tection d'anomalies sans ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç D√©tection d'anomalies (r√®gles basiques)\n",
            "\n",
            "\n",
            "‚úÖ Aucune anomalie d√©tect√©e avec les r√®gles basiques\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mp/code/formations/elasticsearch_cookbook/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/home/mp/code/formations/elasticsearch_cookbook/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# D√©tection d'anomalies basiques bas√©es sur des r√®gles\n",
        "def detect_anomalies_basic():\n",
        "    \"\"\"D√©tecte des anomalies sans ML\"\"\"\n",
        "    anomalies = []\n",
        "    \n",
        "    # 1. IPs sources avec beaucoup d'alertes diff√©rentes (scan suspect)\n",
        "    query = {\n",
        "        \"size\": 0,\n",
        "        \"query\": {\"term\": {\"event_type\": \"alert\"}},\n",
        "        \"aggs\": {\n",
        "            \"suspicious_ips\": {\n",
        "                \"terms\": {\"field\": \"src_ip\", \"size\": 10},\n",
        "                \"aggs\": {\n",
        "                    \"unique_signatures\": {\"cardinality\": {\"field\": \"alert.signature_id\"}},\n",
        "                    \"unique_dest_ips\": {\"cardinality\": {\"field\": \"dest_ip\"}},\n",
        "                    \"total_alerts\": {\"value_count\": {\"field\": \"event_type\"}}\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    result = es.search(index=index_name, body=query)\n",
        "    \n",
        "    print(\"üîç D√©tection d'anomalies (r√®gles basiques)\\n\")\n",
        "    \n",
        "    for bucket in result['aggregations']['suspicious_ips']['buckets']:\n",
        "        ip = bucket['key']\n",
        "        unique_sigs = bucket['unique_signatures']['value']\n",
        "        unique_dests = bucket['unique_dest_ips']['value']\n",
        "        total = bucket['total_alerts']['value']\n",
        "        \n",
        "        # Crit√®res d'anomalie\n",
        "        if unique_sigs > 3 or unique_dests > 5:\n",
        "            anomalies.append({\n",
        "                \"ip\": ip,\n",
        "                \"type\": \"Scan suspect\",\n",
        "                \"signatures\": unique_sigs,\n",
        "                \"destinations\": unique_dests,\n",
        "                \"total\": total\n",
        "            })\n",
        "            print(f\"üö® {ip}: {unique_sigs} signatures, {unique_dests} destinations ({total} alertes)\")\n",
        "    \n",
        "    # 2. Alertes de haute s√©v√©rit√© r√©centes\n",
        "    time_threshold = datetime.now() - timedelta(hours=1)\n",
        "    query_critical = {\n",
        "        \"bool\": {\n",
        "            \"must\": [\n",
        "                {\"term\": {\"event_type\": \"alert\"}},\n",
        "                {\"range\": {\"alert.severity\": {\"lte\": 1}}}\n",
        "            ],\n",
        "            \"filter\": [\n",
        "                {\"range\": {\"@timestamp\": {\"gte\": time_threshold.isoformat()}}}\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    critical = es.search(index=index_name, query=query_critical, size=10)\n",
        "    if critical['hits']['total']['value'] > 0:\n",
        "        print(f\"\\nüî¥ Alertes critiques r√©centes: {critical['hits']['total']['value']}\")\n",
        "        for hit in critical['hits']['hits'][:3]:\n",
        "            src = hit['_source']\n",
        "            alert = src.get('alert', {})\n",
        "            print(f\"   - {alert.get('signature', 'N/A')} (S√©v: {alert.get('severity', 'N/A')})\")\n",
        "    \n",
        "    return anomalies\n",
        "\n",
        "anomalies = detect_anomalies_basic()\n",
        "if not anomalies:\n",
        "    print(\"\\n‚úÖ Aucune anomalie d√©tect√©e avec les r√®gles basiques\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. D√©tection d'anomalies avec ML (Isolation Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Aucune donn√©e d'alerte disponible pour l'analyse ML\n",
            "‚ùå Impossible de pr√©parer les donn√©es\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mp/code/formations/elasticsearch_cookbook/.venv/lib/python3.14/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Pr√©paration des donn√©es pour ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def prepare_features():\n",
        "    \"\"\"Extrait et pr√©pare les features depuis Elasticsearch\"\"\"\n",
        "    # Agr√©gation par IP source avec statistiques\n",
        "    query = {\n",
        "        \"size\": 0,\n",
        "        \"query\": {\"term\": {\"event_type\": \"alert\"}},\n",
        "        \"aggs\": {\n",
        "            \"by_ip\": {\n",
        "                \"terms\": {\"field\": \"src_ip\", \"size\": 100},\n",
        "                \"aggs\": {\n",
        "                    \"avg_severity\": {\"avg\": {\"field\": \"alert.severity\"}},\n",
        "                    \"unique_signatures\": {\"cardinality\": {\"field\": \"alert.signature_id\"}},\n",
        "                    \"unique_dest_ips\": {\"cardinality\": {\"field\": \"dest_ip\"}},\n",
        "                    \"unique_dest_ports\": {\"cardinality\": {\"field\": \"dest_port\"}},\n",
        "                    \"total_alerts\": {\"value_count\": {\"field\": \"event_type\"}}\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    result = es.search(index=index_name, body=query)\n",
        "    \n",
        "    # Construction du DataFrame\n",
        "    data = []\n",
        "    for bucket in result['aggregations']['by_ip']['buckets']:\n",
        "        data.append({\n",
        "            'ip': bucket['key'],\n",
        "            'avg_severity': bucket['avg_severity']['value'] or 0,\n",
        "            'unique_signatures': bucket['unique_signatures']['value'],\n",
        "            'unique_dest_ips': bucket['unique_dest_ips']['value'],\n",
        "            'unique_dest_ports': bucket['unique_dest_ports']['value'],\n",
        "            'total_alerts': bucket['total_alerts']['value']\n",
        "        })\n",
        "    \n",
        "    if not data:\n",
        "        print(\"‚ö†Ô∏è  Aucune donn√©e d'alerte disponible pour l'analyse ML\")\n",
        "        return None, None\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    return df, df[['avg_severity', 'unique_signatures', 'unique_dest_ips', 'unique_dest_ports', 'total_alerts']]\n",
        "\n",
        "df, features = prepare_features()\n",
        "\n",
        "if df is not None and len(df) > 0:\n",
        "    print(f\"üìä {len(df)} IPs analys√©es\")\n",
        "    print(f\"üìà Features: avg_severity, unique_signatures, unique_dest_ips, unique_dest_ports, total_alerts\")\n",
        "else:\n",
        "    print(\"‚ùå Impossible de pr√©parer les donn√©es\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Pas assez de donn√©es pour l'analyse ML\n"
          ]
        }
      ],
      "source": [
        "# Application d'Isolation Forest\n",
        "if df is not None and len(df) > 0:\n",
        "    # Normalisation\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    \n",
        "    # Isolation Forest\n",
        "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "    predictions = iso_forest.fit_predict(features_scaled)\n",
        "    \n",
        "    # Ajout des pr√©dictions au DataFrame\n",
        "    df['anomaly_score'] = iso_forest.score_samples(features_scaled)\n",
        "    df['is_anomaly'] = predictions == -1\n",
        "    \n",
        "    # Affichage des anomalies d√©tect√©es\n",
        "    anomalies_ml = df[df['is_anomaly']].sort_values('anomaly_score')\n",
        "    \n",
        "    print(f\"\\nüîç D√©tection d'anomalies avec Isolation Forest\")\n",
        "    print(f\"üìä {len(anomalies_ml)} anomalies d√©tect√©es sur {len(df)} IPs\\n\")\n",
        "    \n",
        "    if len(anomalies_ml) > 0:\n",
        "        print(\"üö® IPs anormales (top 10):\")\n",
        "        for idx, row in anomalies_ml.head(10).iterrows():\n",
        "            print(f\"   {row['ip']}: score={row['anomaly_score']:.2f}, \"\n",
        "                  f\"signatures={row['unique_signatures']}, \"\n",
        "                  f\"destinations={row['unique_dest_ips']}, \"\n",
        "                  f\"alertes={row['total_alerts']}\")\n",
        "    else:\n",
        "        print(\"‚úÖ Aucune anomalie d√©tect√©e par ML\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Pas assez de donn√©es pour l'analyse ML\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R√©sum√©\n",
        "\n",
        "- ‚úÖ Stack v√©rifi√©e et fonctionnelle\n",
        "- ‚úÖ Donn√©es index√©es et analysables\n",
        "- ‚úÖ D√©tection d'anomalies basiques op√©rationnelle\n",
        "- ‚úÖ D√©tection d'anomalies ML (Isolation Forest) op√©rationnelle\n",
        "\n",
        "**Prochaines √©tapes :**\n",
        "- Explorer les donn√©es dans Kibana\n",
        "- Affiner les r√®gles de d√©tection\n",
        "- Ajuster les param√®tres ML (contamination, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
